{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap redfin.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_image(image_url, save_path):\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {image_url}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def get_image_urls(property_url):\n",
    "    base_url = \"https://www.redfin.com\"\n",
    "    property_id = property_url.split(\"/\")[-1]\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'google',\n",
    "        'Accept': 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.get(f\"{base_url}/stingray/api/home/details/aboveTheFold\",\n",
    "                            headers=headers,\n",
    "                            params={'propertyId': property_id, 'accessLevel': 1})\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Status code is not 200. Got {response.status_code} instead.\\n\"\n",
    "                        f\"Property ID: {property_id}\\n\"\n",
    "                        f\"Data: {response.text}\\n\"\n",
    "                        f\"Status text: {response.reason}\")\n",
    "\n",
    "    data = response.text[4:]\n",
    "    json_data = json.loads(data)\n",
    "    above_the_fold_details = json_data['payload']['mediaBrowserInfo']['photos'] if 'payload' in json_data else None\n",
    "    \n",
    "    if not above_the_fold_details:\n",
    "        raise Exception(\"No data found\")\n",
    "\n",
    "    image_urls = [photo['photoUrls']['fullScreenPhotoUrl'] for photo in above_the_fold_details]\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "\n",
    "\n",
    "def download_images_for_properties(property_urls, save_dir):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "\n",
    "        for property_url in property_urls:\n",
    "            property_id = property_url.split(\"/\")[-1]\n",
    "            try:\n",
    "                image_urls = get_image_urls(property_url)\n",
    "                for i, url in enumerate(image_urls):\n",
    "                    filename = f\"{property_id}_{i}.jpg\"\n",
    "                    save_path = os.path.join(save_dir, filename)\n",
    "                    futures.append(executor.submit(download_image, url, save_path))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result() \n",
    "            except Exception as e:\n",
    "                print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "redfin_urls = [\n",
    "  \n",
    "    ]\n",
    "    \n",
    "save_dir = \"redfin_images\"  \n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "download_images_for_properties(redfin_urls, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293\n",
      "Duplicate removal complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "def file_hash(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "def remove_duplicates(directory):\n",
    "    unique_files = {}\n",
    "    duplicates = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.jpg'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            filehash = file_hash(filepath)\n",
    "\n",
    "            if filehash in unique_files:\n",
    "                duplicates.append(filepath)\n",
    "            else:\n",
    "                unique_files[filehash] = filename\n",
    "\n",
    "    cnt=0\n",
    "    for filepath in duplicates:\n",
    "        os.remove(filepath)\n",
    "        cnt+=1\n",
    "    print(cnt)\n",
    "\n",
    "    print(\"Duplicate removal complete.\")\n",
    "\n",
    "directory = 'redfin_images'  \n",
    "remove_duplicates(directory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zip2zip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
