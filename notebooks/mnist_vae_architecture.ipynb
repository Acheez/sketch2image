{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xquyn7QuG1-Z"
      },
      "source": [
        "# Install libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "695_L-O34rtF"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGqxQKzeUZmU"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils\n",
        "import torch.distributions\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-z9pHE5gGpr"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFeY-ZrL4m4p"
      },
      "source": [
        "# Unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfkM-wSjd3nr",
        "outputId": "87784a57-feb6-43be-f2d5-625a12cce6ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv7kaVPAxGsa"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/Bachelor's Project/data/original/redfin_images.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25_jPvR24lQ4"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJg4eh930tSy"
      },
      "outputs": [],
      "source": [
        "image_size = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbvUQMrU4dIs"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3_NWmVvUuA6"
      },
      "outputs": [],
      "source": [
        "class VariationalEncoder(nn.Module):\n",
        "    def __init__(self, latent_dims, image_size):\n",
        "        super(VariationalEncoder, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.linear1 = nn.Linear(self.image_size * self.image_size, 512)\n",
        "        nn.init.xavier_uniform_(self.linear1.weight)\n",
        "        self.bn1 = nn.BatchNorm1d(512)  # Add batch normalization\n",
        "        self.linear2 = nn.Linear(512, latent_dims)\n",
        "        nn.init.xavier_uniform_(self.linear2.weight)\n",
        "        self.bn2 = nn.BatchNorm1d(latent_dims)  # Add batch normalization\n",
        "        self.linear3 = nn.Linear(512, latent_dims)\n",
        "        nn.init.xavier_uniform_(self.linear3.weight)\n",
        "        self.bn3 = nn.BatchNorm1d(latent_dims)  # Add batch normalization\n",
        "        self.N = torch.distributions.Normal(0, 1)\n",
        "        self.N.loc = self.N.loc.cuda()\n",
        "        self.N.scale = self.N.scale.cuda()\n",
        "        self.kl = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.bn1(self.linear1(x)))  # Apply batch normalization\n",
        "        mu = self.bn2(self.linear2(x))  # Apply batch normalization\n",
        "        sigma = self.bn3(self.linear3(x))  # Apply batch normalization\n",
        "        sigma = torch.clamp(sigma, min=1e-2, max=1e2)\n",
        "        z = mu + sigma * self.N.sample(mu.shape)\n",
        "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0b6SmJ5aWPj"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dims, image_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.linear1 = nn.Linear(latent_dims, 512)\n",
        "        nn.init.xavier_uniform_(self.linear1.weight)\n",
        "        self.bn1 = nn.BatchNorm1d(512)  # Add batch normalization\n",
        "        self.linear2 = nn.Linear(512, self.image_size * self.image_size)\n",
        "        nn.init.xavier_uniform_(self.linear2.weight)\n",
        "        self.bn2 = nn.BatchNorm1d(self.image_size * self.image_size)  # Add batch normalization\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = F.relu(self.bn1(self.linear1(z)))  # Apply batch normalization\n",
        "        z = torch.sigmoid(self.bn2(self.linear2(z)))  # Apply batch normalization\n",
        "        return z.reshape((-1, 1, self.image_size, self.image_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpsBg5dJabKx"
      },
      "outputs": [],
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dims, image_size):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = VariationalEncoder(latent_dims, image_size=image_size)\n",
        "        self.decoder = Decoder(latent_dims, image_size=image_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x333vsgD6XV"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2heL5dUmjQLf"
      },
      "outputs": [],
      "source": [
        "def plot_reconstructed(autoencoder, r0=(-5, 10), r1=(-10, 5), n=12):\n",
        "    w = 256\n",
        "    img = np.zeros((n*w, n*w))\n",
        "    for i, y in enumerate(np.linspace(*r1, n)):\n",
        "        for j, x in enumerate(np.linspace(*r0, n)):\n",
        "            z = torch.Tensor([[x, y]]).to(device)\n",
        "            x_hat = autoencoder.decoder(z)\n",
        "            x_hat = x_hat.reshape(256, 256).to('cpu').detach().numpy()\n",
        "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n",
        "    plt.imshow(img, extent=[*r0, *r1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeyYSptXjSED"
      },
      "outputs": [],
      "source": [
        "def plot_latent(autoencoder, data, num_batches=100):\n",
        "    for i, (x, y) in enumerate(data):\n",
        "        z = autoencoder.encoder(x.to(device))\n",
        "        z = z.to('cpu').detach().numpy()\n",
        "        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10')\n",
        "        if i > num_batches:\n",
        "            plt.colorbar()\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jklwxz_VERnX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "def plot_images(original_images, reconstructed_images, device):\n",
        "    \"\"\"\n",
        "    Plots the original and reconstructed images.\n",
        "\n",
        "    Args:\n",
        "        original_images (Tensor): Batch of original images.\n",
        "        reconstructed_images (Tensor): Batch of reconstructed images.\n",
        "        device (torch.device): Device where the data resides.\n",
        "    \"\"\"\n",
        "    # Move the images to CPU for plotting\n",
        "    original_images = original_images.cpu()\n",
        "    reconstructed_images = reconstructed_images.cpu()\n",
        "\n",
        "    # Create a grid of images\n",
        "    image_grid = vutils.make_grid(original_images, nrow=8, normalize=True)\n",
        "    recon_grid = vutils.make_grid(reconstructed_images, nrow=8, normalize=True)\n",
        "\n",
        "    # Plot the images\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0))\n",
        "    plt.title('Original Images')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(recon_grid.permute(1, 2, 0))\n",
        "    plt.title('Reconstructed Images')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAM3Lx5cgUfv"
      },
      "source": [
        "# Training script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIlz3BbKm6Ii"
      },
      "outputs": [],
      "source": [
        "from tqdm.autonotebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvoGxCWTgXVd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def train(autoencoder, data, epochs=20):\n",
        "    opt = torch.optim.Adam(autoencoder.parameters())\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=100, gamma=0.1)  # Learning rate scheduler\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    autoencoder = autoencoder.to(device)\n",
        "    loss_val = 0\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for batch_idx, x in enumerate(tqdm(data, total=len(data))):\n",
        "            x = x.to(device)\n",
        "            opt.zero_grad()\n",
        "\n",
        "            x_hat = autoencoder(x)\n",
        "            loss = ((x - x_hat)**2).sum() + autoencoder.encoder.kl\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            scheduler.step()\n",
        "            loss_val += loss.item()\n",
        "            # Visualize original and reconstructed images\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(data)}], Loss: {loss_val / 100:.4f}')\n",
        "                loss_val = 0\n",
        "                with torch.no_grad():\n",
        "                    original_images = make_grid(x[:8], nrow=8, normalize=True)\n",
        "                    reconstructed_images = make_grid(x_hat[:8], nrow=8, normalize=True)\n",
        "\n",
        "                    # Log or save images\n",
        "                    # (Implementation depends on your specific use case)\n",
        "                    plot_images(original_images, reconstructed_images, device)\n",
        "\n",
        "    return autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfGprz8sggku"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4UT8Berk1gf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TxJr7rRghgt"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.image_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.data_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk53Wd8xghtH"
      },
      "source": [
        "# wafaw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgZuCzONgXtI"
      },
      "outputs": [],
      "source": [
        "image_size=256\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),  # Resize images to 28x28 pixels\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize image tensors\n",
        "])\n",
        "\n",
        "# dataset = CustomImageDataset('/content/redfin_images', transform=transform)\n",
        "dataset = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.MNIST('./data',\n",
        "               transform=torchvision.transforms.ToTensor(),\n",
        "               download=True),\n",
        "        batch_size=128,\n",
        "        shuffle=True)\n",
        "data_loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "vae = VariationalAutoencoder(128, image_size=image_size).to(device) # GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "5e225ac1785b41b7a520985e27c08e71",
            "5ffa725821e9431088324d60d0fbdf98"
          ]
        },
        "id": "fHQLdea7laFI",
        "outputId": "a6f90ddd-66f1-40f6-8c46-892c978aab87"
      },
      "outputs": [],
      "source": [
        "vae = train(vae, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWyvVb16l80U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}